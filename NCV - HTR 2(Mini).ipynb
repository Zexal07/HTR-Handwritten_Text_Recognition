{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2bf11a5-0d41-4c6b-bc3b-dbc2620d7d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "✅ Loaded 330396 valid records for training.\n",
      "\n",
      "============================================================\n",
      "MINI DATASET TRAINING (IMPROVED)\n",
      "============================================================\n",
      "Mini dataset size: 5000 samples\n",
      "Training mini model for 10 epochs...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmed\\anaconda3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN output width: 64\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [07:04<00:00,  2.70s/it, batch_loss=3.2764, min_loss=2.9628]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest Loss in Epoch: 2.9628\n",
      "\n",
      "✅ Best model saved! Lowest loss: 2.9628\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:14<00:00,  2.38s/it, batch_loss=3.0321, min_loss=2.7900]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest Loss in Epoch: 2.7900\n",
      "\n",
      "✅ Best model saved! Lowest loss: 2.7900\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:57<00:00,  2.66s/it, batch_loss=2.9452, min_loss=2.7173]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest Loss in Epoch: 2.7173\n",
      "\n",
      "✅ Best model saved! Lowest loss: 2.7173\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:09<00:00,  2.35s/it, batch_loss=3.3055, min_loss=2.6923]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest Loss in Epoch: 2.6923\n",
      "\n",
      "✅ Best model saved! Lowest loss: 2.6923\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:08<00:00,  2.35s/it, batch_loss=2.7364, min_loss=2.6364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest Loss in Epoch: 2.6364\n",
      "\n",
      "✅ Best model saved! Lowest loss: 2.6364\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:11<00:00,  2.37s/it, batch_loss=2.8129, min_loss=2.6192]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest Loss in Epoch: 2.6192\n",
      "\n",
      "✅ Best model saved! Lowest loss: 2.6192\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:10<00:00,  2.36s/it, batch_loss=2.8105, min_loss=2.5785]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest Loss in Epoch: 2.5785\n",
      "\n",
      "✅ Best model saved! Lowest loss: 2.5785\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:10<00:00,  2.36s/it, batch_loss=2.8365, min_loss=2.5585]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest Loss in Epoch: 2.5585\n",
      "\n",
      "✅ Best model saved! Lowest loss: 2.5585\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:07<00:00,  2.34s/it, batch_loss=2.9176, min_loss=2.5796]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest Loss in Epoch: 2.5796\n",
      "\n",
      "Loss did not improve. Best so far: 2.5585\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:04<00:00,  2.32s/it, batch_loss=2.8161, min_loss=2.5347]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest Loss in Epoch: 2.5347\n",
      "\n",
      "✅ Best model saved! Lowest loss: 2.5347\n",
      "\n",
      "\n",
      "✅ Training complete! Best loss: 2.5347\n",
      "Model saved to: C:\\Users\\ahmed\\Desktop\\Ahmed Sajid\\Office - NCV\\NCV - HTR\\htr_crnn_mini.pth\n",
      "\n",
      "Loss history by epoch:\n",
      "  Epoch 1: 2.9628\n",
      "  Epoch 2: 2.7900\n",
      "  Epoch 3: 2.7173\n",
      "  Epoch 4: 2.6923\n",
      "  Epoch 5: 2.6364\n",
      "  Epoch 6: 2.6192\n",
      "  Epoch 7: 2.5785\n",
      "  Epoch 8: 2.5585\n",
      "  Epoch 9: 2.5796\n",
      "  Epoch 10: 2.5347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "class Config:\n",
    "    CHARS = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 .,-?!&\"\n",
    "    VOCAB_SIZE = len(CHARS) + 1\n",
    "    \n",
    "    BATCH_SIZE = 32\n",
    "    NUM_EPOCHS = 10\n",
    "    LEARNING_RATE = 1e-3\n",
    "    \n",
    "    IMG_HEIGHT = 64\n",
    "    IMG_WIDTH = 256\n",
    "    HIDDEN_SIZE = 256\n",
    "    \n",
    "    BASE_DIR = os.getcwd()\n",
    "    CSV_DIR = os.path.join(BASE_DIR, \"CSV\")\n",
    "    TRAIN_IMG_DIR = os.path.join(BASE_DIR, \"train_v2/train\")\n",
    "    \n",
    "    TRAIN_CSV = os.path.join(CSV_DIR, \"written_name_train.csv\")\n",
    "    \n",
    "    MINI_MODEL_PATH = os.path.join(BASE_DIR, \"htr_crnn_mini.pth\")\n",
    "\n",
    "\n",
    "def create_char_to_int_mapping(chars: str) -> Tuple[Dict[str, int], Dict[int, str]]:\n",
    "    char_to_int = {char: i + 1 for i, char in enumerate(chars)}\n",
    "    int_to_char = {i + 1: char for i, char in enumerate(chars)}\n",
    "    char_to_int['CTC_BLANK'] = 0\n",
    "    int_to_char[0] = ''\n",
    "    return char_to_int, int_to_char\n",
    "\n",
    "CHAR_TO_INT, INT_TO_CHAR = create_char_to_int_mapping(Config.CHARS)\n",
    "\n",
    "\n",
    "def load_all_annotations() -> Dict[str, Tuple[List[Tuple[str, str]], str]]:\n",
    "    csv_path = Config.TRAIN_CSV\n",
    "    img_dir = Config.TRAIN_IMG_DIR\n",
    "    \n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"❌ CSV not found at {csv_path}\")\n",
    "        return {}\n",
    "    \n",
    "    df = pd.read_csv(csv_path)\n",
    "    df.columns = ['image_filename', 'transcription_text']\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    df['exists'] = df['image_filename'].apply(lambda x: os.path.exists(os.path.join(img_dir, x)))\n",
    "    df = df[df['exists']].drop(columns=['exists'])\n",
    "\n",
    "    annotations = list(df[['image_filename', 'transcription_text']].itertuples(index=False, name=None))\n",
    "    print(f\"✅ Loaded {len(annotations)} valid records for training.\")\n",
    "    \n",
    "    return {'train': (annotations, img_dir)}\n",
    "\n",
    "\n",
    "class HTRDataset(Dataset):\n",
    "    def __init__(self, img_dir: str, annotations: List[Tuple[str, str]], transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.annotations = annotations\n",
    "        self.transform = transform\n",
    "        self.char_to_int = CHAR_TO_INT\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_filename, label_text = self.annotations[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_filename)\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception:\n",
    "            image = Image.new('RGB', (Config.IMG_WIDTH, Config.IMG_HEIGHT), color='black')\n",
    "            \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label_encoded = [self.char_to_int.get(char, 0) for char in label_text]\n",
    "        target = torch.tensor(label_encoded, dtype=torch.long)\n",
    "        target_len = torch.tensor(len(target), dtype=torch.long)\n",
    "        \n",
    "        return image, target, target_len\n",
    "\n",
    "\n",
    "def collate_fn(batch, model_cnn_output_width=None):\n",
    "    images, targets, target_lengths = zip(*batch)\n",
    "    images = torch.stack(images, 0)\n",
    "    \n",
    "    max_target_len = max(target_lengths)\n",
    "    padded_targets = torch.zeros((len(targets), max_target_len), dtype=torch.long)\n",
    "    for i, target in enumerate(targets):\n",
    "        padded_targets[i, :target.size(0)] = target\n",
    "    \n",
    "    target_lengths = torch.stack(target_lengths)\n",
    "    \n",
    "    if model_cnn_output_width is None:\n",
    "        model_cnn_output_width = Config.IMG_WIDTH // 4\n",
    "    \n",
    "    input_lengths = torch.full((len(batch),), model_cnn_output_width, dtype=torch.long)\n",
    "    \n",
    "    return images, padded_targets, input_lengths, target_lengths\n",
    "\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, img_height, vocab_size, hidden_size):\n",
    "        super(CRNN, self).__init__()\n",
    "        \n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 1), (2, 1)),\n",
    "            nn.Conv2d(256, 512, 3, padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, None))\n",
    "        )\n",
    "        \n",
    "        self.map_to_rnn = nn.Linear(512, hidden_size)\n",
    "        \n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=3,\n",
    "            bidirectional=True,\n",
    "            dropout=0.3,\n",
    "            batch_first=False\n",
    "        )\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size * 2, vocab_size)\n",
    "        \n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        cnn_out = self.cnn(x)\n",
    "        cnn_out = cnn_out.squeeze(2)\n",
    "        cnn_out = cnn_out.permute(0, 2, 1)\n",
    "        \n",
    "        rnn_input = self.map_to_rnn(cnn_out)\n",
    "        rnn_input = rnn_input.permute(1, 0, 2)\n",
    "        \n",
    "        rnn_out, _ = self.rnn(rnn_input)\n",
    "        output = self.linear(rnn_out)\n",
    "        output = nn.functional.log_softmax(output, dim=2)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def get_cnn_output_width(self, batch_size=1, device='cpu'):\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(batch_size, 3, Config.IMG_HEIGHT, Config.IMG_WIDTH, device=device)\n",
    "            cnn_out = self.cnn(dummy_input)\n",
    "            return cnn_out.size(-1)\n",
    "\n",
    "\n",
    "def train_htr_model(data_loader, model, criterion, optimizer, device, scheduler=None):\n",
    "    model.train()\n",
    "    min_loss = float('inf')\n",
    "    \n",
    "    pbar = tqdm(data_loader, desc=\"Training\")\n",
    "    for images, targets, input_lengths, target_lengths in pbar:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = criterion(output, targets, input_lengths.to(device), target_lengths.to(device))\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_loss = loss.item()\n",
    "        min_loss = min(min_loss, batch_loss)\n",
    "        pbar.set_postfix({'batch_loss': f'{batch_loss:.4f}', 'min_loss': f'{min_loss:.4f}'})\n",
    "\n",
    "    return min_loss\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\\n\")\n",
    "    \n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((Config.IMG_HEIGHT, Config.IMG_WIDTH)),\n",
    "        transforms.RandomAffine(degrees=3, translate=(0.02, 0.02), shear=2),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    \n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((Config.IMG_HEIGHT, Config.IMG_WIDTH)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    \n",
    "    all_data = load_all_annotations()\n",
    "    \n",
    "    if 'train' not in all_data:\n",
    "        print(\"❌ Training data not found!\")\n",
    "        exit(1)\n",
    "    \n",
    "    train_annotations, train_img_dir = all_data['train']\n",
    "    train_dataset = HTRDataset(img_dir=train_img_dir, annotations=train_annotations, transform=train_transform)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MINI DATASET TRAINING (IMPROVED)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    mini_size = 5000\n",
    "    mini_indices = torch.randperm(len(train_dataset))[:mini_size].tolist()\n",
    "    mini_dataset = Subset(train_dataset, mini_indices)\n",
    "    \n",
    "    mini_loader = DataLoader(\n",
    "        mini_dataset,\n",
    "        batch_size=Config.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    print(f\"Mini dataset size: {len(mini_dataset)} samples\")\n",
    "    print(f\"Training mini model for {Config.NUM_EPOCHS} epochs...\\n\")\n",
    "    \n",
    "    model_mini = CRNN(Config.IMG_HEIGHT, Config.VOCAB_SIZE, Config.HIDDEN_SIZE).to(device)\n",
    "    \n",
    "    cnn_output_width = model_mini.get_cnn_output_width(batch_size=1, device=device)\n",
    "    print(f\"CNN output width: {cnn_output_width}\")\n",
    "    \n",
    "    def collate_fn_with_width(batch):\n",
    "        return collate_fn(batch, model_cnn_output_width=cnn_output_width)\n",
    "    \n",
    "    criterion = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
    "    optimizer_mini = optim.Adam(model_mini.parameters(), lr=Config.LEARNING_RATE)\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer_mini, mode='min', factor=0.5, patience=3, verbose=True\n",
    "    )\n",
    "    \n",
    "    best_loss_mini = float('inf')\n",
    "    loss_history = []\n",
    "    \n",
    "    for epoch in range(1, Config.NUM_EPOCHS + 1):\n",
    "        print(f\"Epoch {epoch}/{Config.NUM_EPOCHS}\")\n",
    "        min_epoch_loss = train_htr_model(mini_loader, model_mini, criterion, optimizer_mini, device, scheduler)\n",
    "        print(f\"Lowest Loss in Epoch: {min_epoch_loss:.4f}\\n\")\n",
    "        \n",
    "        loss_history.append(min_epoch_loss)\n",
    "        \n",
    "        scheduler.step(min_epoch_loss)\n",
    "        \n",
    "        if min_epoch_loss < best_loss_mini:\n",
    "            best_loss_mini = min_epoch_loss\n",
    "            torch.save(model_mini.state_dict(), Config.MINI_MODEL_PATH)\n",
    "            print(f\"✅ Best model saved! Lowest loss: {best_loss_mini:.4f}\\n\")\n",
    "        else:\n",
    "            print(f\"Loss did not improve. Best so far: {best_loss_mini:.4f}\\n\")\n",
    "    \n",
    "    print(f\"\\n✅ Training complete! Best loss: {best_loss_mini:.4f}\")\n",
    "    print(f\"Model saved to: {Config.MINI_MODEL_PATH}\")\n",
    "    print(f\"\\nLoss history by epoch:\")\n",
    "    for epoch, loss in enumerate(loss_history, 1):\n",
    "        print(f\"  Epoch {epoch}: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b61e70b-4000-43a9-a929-28c7c867c879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "✅ Loaded 330396 valid records for training.\n",
      "\n",
      "============================================================\n",
      "CONTINUE TRAINING FROM SAVED MODEL (IMPROVED)\n",
      "============================================================\n",
      "Mini dataset size: 5000 samples\n",
      "\n",
      "Loading existing model from: C:\\Users\\ahmed\\Desktop\\Ahmed Sajid\\Office - NCV\\NCV - HTR\\htr_crnn_mini.pth\n",
      "✅ Model loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmed\\AppData\\Local\\Temp\\ipykernel_12428\\1221645751.py:250: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_mini.load_state_dict(torch.load(Config.MINI_MODEL_PATH, map_location=device))\n",
      "C:\\Users\\ahmed\\anaconda3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN output width: 64\n",
      "\n",
      "Continuing training for 50 more epochs...\n",
      "\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [07:14<00:00,  2.77s/it, batch_loss=2.2780, min_loss=2.0026]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 2.0026\n",
      "\n",
      "✅ Best model saved! Lowest loss: 2.0026\n",
      "\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:03<00:00,  2.31s/it, batch_loss=2.0940, min_loss=1.7183]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 1.7183\n",
      "\n",
      "✅ Best model saved! Lowest loss: 1.7183\n",
      "\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:04<00:00,  2.32s/it, batch_loss=1.9384, min_loss=1.7456]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 1.7456\n",
      "\n",
      "Loss did not improve. Best so far: 1.7183\n",
      "\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:10<00:00,  2.36s/it, batch_loss=1.7950, min_loss=1.6372]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 1.6372\n",
      "\n",
      "✅ Best model saved! Lowest loss: 1.6372\n",
      "\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:03<00:00,  2.32s/it, batch_loss=2.1004, min_loss=1.5689]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 1.5689\n",
      "\n",
      "✅ Best model saved! Lowest loss: 1.5689\n",
      "\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:02<00:00,  2.31s/it, batch_loss=1.3110, min_loss=1.3110]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 1.3110\n",
      "\n",
      "✅ Best model saved! Lowest loss: 1.3110\n",
      "\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [05:59<00:00,  2.29s/it, batch_loss=1.0817, min_loss=1.0817]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 1.0817\n",
      "\n",
      "✅ Best model saved! Lowest loss: 1.0817\n",
      "\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:04<00:00,  2.32s/it, batch_loss=0.7769, min_loss=0.7769]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 0.7769\n",
      "\n",
      "✅ Best model saved! Lowest loss: 0.7769\n",
      "\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:03<00:00,  2.32s/it, batch_loss=1.2977, min_loss=1.0491]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 1.0491\n",
      "\n",
      "Loss did not improve. Best so far: 0.7769\n",
      "\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:04<00:00,  2.32s/it, batch_loss=1.7001, min_loss=0.9573]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 0.9573\n",
      "\n",
      "Loss did not improve. Best so far: 0.7769\n",
      "\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [05:58<00:00,  2.28s/it, batch_loss=0.8827, min_loss=0.8418]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 0.8418\n",
      "\n",
      "Loss did not improve. Best so far: 0.7769\n",
      "\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:45<00:00,  2.58s/it, batch_loss=1.2038, min_loss=0.7306]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 0.7306\n",
      "\n",
      "✅ Best model saved! Lowest loss: 0.7306\n",
      "\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:00<00:00,  2.30s/it, batch_loss=0.9055, min_loss=0.6433]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 0.6433\n",
      "\n",
      "✅ Best model saved! Lowest loss: 0.6433\n",
      "\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:00<00:00,  2.29s/it, batch_loss=0.8707, min_loss=0.6023]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 0.6023\n",
      "\n",
      "✅ Best model saved! Lowest loss: 0.6023\n",
      "\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:03<00:00,  2.31s/it, batch_loss=0.5231, min_loss=0.4837]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 0.4837\n",
      "\n",
      "✅ Best model saved! Lowest loss: 0.4837\n",
      "\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [05:59<00:00,  2.29s/it, batch_loss=0.8403, min_loss=0.5146]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 0.5146\n",
      "\n",
      "Loss did not improve. Best so far: 0.4837\n",
      "\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [05:54<00:00,  2.26s/it, batch_loss=1.1016, min_loss=0.4511]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 0.4511\n",
      "\n",
      "✅ Best model saved! Lowest loss: 0.4511\n",
      "\n",
      "Epoch 18/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [05:50<00:00,  2.23s/it, batch_loss=0.5183, min_loss=0.3997]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 0.3997\n",
      "\n",
      "✅ Best model saved! Lowest loss: 0.3997\n",
      "\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [05:49<00:00,  2.23s/it, batch_loss=0.5597, min_loss=0.3780]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 0.3780\n",
      "\n",
      "✅ Best model saved! Lowest loss: 0.3780\n",
      "\n",
      "Epoch 20/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [05:50<00:00,  2.23s/it, batch_loss=0.6454, min_loss=0.3830]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 0.3830\n",
      "\n",
      "Loss did not improve. Best so far: 0.3780\n",
      "\n",
      "Epoch 21/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [05:50<00:00,  2.23s/it, batch_loss=1.0981, min_loss=0.3639]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 0.3639\n",
      "\n",
      "✅ Best model saved! Lowest loss: 0.3639\n",
      "\n",
      "Epoch 22/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [05:50<00:00,  2.23s/it, batch_loss=0.7336, min_loss=0.3315]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 0.3315\n",
      "\n",
      "✅ Best model saved! Lowest loss: 0.3315\n",
      "\n",
      "Epoch 23/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [05:49<00:00,  2.23s/it, batch_loss=0.2770, min_loss=0.2770]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 0.2770\n",
      "\n",
      "✅ Best model saved! Lowest loss: 0.2770\n",
      "\n",
      "Epoch 24/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:09<00:00,  2.35s/it, batch_loss=0.8712, min_loss=0.2339]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 0.2339\n",
      "\n",
      "✅ Best model saved! Lowest loss: 0.2339\n",
      "\n",
      "Epoch 25/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:03<00:00,  2.31s/it, batch_loss=0.1766, min_loss=0.1766]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 0.1766\n",
      "\n",
      "✅ Best model saved! Lowest loss: 0.1766\n",
      "\n",
      "Epoch 26/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:01<00:00,  2.31s/it, batch_loss=0.4016, min_loss=0.1984]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 0.1984\n",
      "\n",
      "Loss did not improve. Best so far: 0.1766\n",
      "\n",
      "Epoch 27/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:00<00:00,  2.29s/it, batch_loss=0.2807, min_loss=0.1894]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 0.1894\n",
      "\n",
      "Loss did not improve. Best so far: 0.1766\n",
      "\n",
      "Epoch 28/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:00<00:00,  2.30s/it, batch_loss=0.7078, min_loss=0.1822]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 0.1822\n",
      "\n",
      "Loss did not improve. Best so far: 0.1766\n",
      "\n",
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:00<00:00,  2.29s/it, batch_loss=0.3325, min_loss=0.2054]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 0.2054\n",
      "\n",
      "Loss did not improve. Best so far: 0.1766\n",
      "\n",
      "Epoch 30/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:00<00:00,  2.30s/it, batch_loss=0.1278, min_loss=0.1225]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 0.1225\n",
      "\n",
      "✅ Best model saved! Lowest loss: 0.1225\n",
      "\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:00<00:00,  2.29s/it, batch_loss=0.6799, min_loss=0.1119]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 0.1119\n",
      "\n",
      "✅ Best model saved! Lowest loss: 0.1119\n",
      "\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:11<00:00,  2.36s/it, batch_loss=0.4329, min_loss=0.0950]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 0.0950\n",
      "\n",
      "✅ Best model saved! Lowest loss: 0.0950\n",
      "\n",
      "Epoch 33/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:04<00:00,  2.32s/it, batch_loss=0.1878, min_loss=0.0927]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 0.0927\n",
      "\n",
      "✅ Best model saved! Lowest loss: 0.0927\n",
      "\n",
      "Epoch 34/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:01<00:00,  2.30s/it, batch_loss=0.2607, min_loss=0.0843]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 0.0843\n",
      "\n",
      "✅ Best model saved! Lowest loss: 0.0843\n",
      "\n",
      "Epoch 35/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:02<00:00,  2.31s/it, batch_loss=0.1416, min_loss=0.0920]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 0.0920\n",
      "\n",
      "Loss did not improve. Best so far: 0.0843\n",
      "\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:00<00:00,  2.30s/it, batch_loss=0.3717, min_loss=0.0760]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 0.0760\n",
      "\n",
      "✅ Best model saved! Lowest loss: 0.0760\n",
      "\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:00<00:00,  2.29s/it, batch_loss=0.2223, min_loss=0.0711]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 0.0711\n",
      "\n",
      "✅ Best model saved! Lowest loss: 0.0711\n",
      "\n",
      "Epoch 38/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:05<00:00,  2.33s/it, batch_loss=0.1480, min_loss=0.0553]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 0.0553\n",
      "\n",
      "✅ Best model saved! Lowest loss: 0.0553\n",
      "\n",
      "Epoch 39/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:02<00:00,  2.31s/it, batch_loss=0.1161, min_loss=0.0638]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 0.0638\n",
      "\n",
      "Loss did not improve. Best so far: 0.0553\n",
      "\n",
      "Epoch 40/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:02<00:00,  2.31s/it, batch_loss=0.0221, min_loss=0.0221]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 0.0221\n",
      "\n",
      "✅ Best model saved! Lowest loss: 0.0221\n",
      "\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:02<00:00,  2.31s/it, batch_loss=0.0407, min_loss=0.0407]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 0.0407\n",
      "\n",
      "Loss did not improve. Best so far: 0.0221\n",
      "\n",
      "Epoch 42/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:02<00:00,  2.31s/it, batch_loss=0.0216, min_loss=0.0216]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 0.0216\n",
      "\n",
      "✅ Best model saved! Lowest loss: 0.0216\n",
      "\n",
      "Epoch 43/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:01<00:00,  2.30s/it, batch_loss=0.0360, min_loss=0.0348]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 0.0348\n",
      "\n",
      "Loss did not improve. Best so far: 0.0216\n",
      "\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:03<00:00,  2.32s/it, batch_loss=0.1306, min_loss=0.0438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 0.0438\n",
      "\n",
      "Loss did not improve. Best so far: 0.0216\n",
      "\n",
      "Epoch 45/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:03<00:00,  2.32s/it, batch_loss=0.0874, min_loss=0.0408]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 0.0408\n",
      "\n",
      "Loss did not improve. Best so far: 0.0216\n",
      "\n",
      "Epoch 46/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:05<00:00,  2.32s/it, batch_loss=0.2020, min_loss=0.0418]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 0.0418\n",
      "\n",
      "Loss did not improve. Best so far: 0.0216\n",
      "\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:04<00:00,  2.32s/it, batch_loss=0.3800, min_loss=0.0437]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 0.0437\n",
      "\n",
      "Loss did not improve. Best so far: 0.0216\n",
      "\n",
      "Epoch 48/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:04<00:00,  2.32s/it, batch_loss=0.2015, min_loss=0.0301]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 0.0301\n",
      "\n",
      "Loss did not improve. Best so far: 0.0216\n",
      "\n",
      "Epoch 49/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:04<00:00,  2.32s/it, batch_loss=0.0181, min_loss=0.0181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 0.0181\n",
      "\n",
      "✅ Best model saved! Lowest loss: 0.0181\n",
      "\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████| 157/157 [06:01<00:00,  2.31s/it, batch_loss=0.1663, min_loss=0.0298]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Loss: 0.0298\n",
      "\n",
      "Loss did not improve. Best so far: 0.0181\n",
      "\n",
      "\n",
      "✅ Continued training complete! Best loss achieved: 0.0181\n",
      "Model saved to: C:\\Users\\ahmed\\Desktop\\Ahmed Sajid\\Office - NCV\\NCV - HTR\\htr_crnn_mini.pth\n",
      "\n",
      "Loss history by epoch:\n",
      "  Epoch 1: 2.0026\n",
      "  Epoch 2: 1.7183\n",
      "  Epoch 3: 1.7456\n",
      "  Epoch 4: 1.6372\n",
      "  Epoch 5: 1.5689\n",
      "  Epoch 6: 1.3110\n",
      "  Epoch 7: 1.0817\n",
      "  Epoch 8: 0.7769\n",
      "  Epoch 9: 1.0491\n",
      "  Epoch 10: 0.9573\n",
      "  Epoch 11: 0.8418\n",
      "  Epoch 12: 0.7306\n",
      "  Epoch 13: 0.6433\n",
      "  Epoch 14: 0.6023\n",
      "  Epoch 15: 0.4837\n",
      "  Epoch 16: 0.5146\n",
      "  Epoch 17: 0.4511\n",
      "  Epoch 18: 0.3997\n",
      "  Epoch 19: 0.3780\n",
      "  Epoch 20: 0.3830\n",
      "  Epoch 21: 0.3639\n",
      "  Epoch 22: 0.3315\n",
      "  Epoch 23: 0.2770\n",
      "  Epoch 24: 0.2339\n",
      "  Epoch 25: 0.1766\n",
      "  Epoch 26: 0.1984\n",
      "  Epoch 27: 0.1894\n",
      "  Epoch 28: 0.1822\n",
      "  Epoch 29: 0.2054\n",
      "  Epoch 30: 0.1225\n",
      "  Epoch 31: 0.1119\n",
      "  Epoch 32: 0.0950\n",
      "  Epoch 33: 0.0927\n",
      "  Epoch 34: 0.0843\n",
      "  Epoch 35: 0.0920\n",
      "  Epoch 36: 0.0760\n",
      "  Epoch 37: 0.0711\n",
      "  Epoch 38: 0.0553\n",
      "  Epoch 39: 0.0638\n",
      "  Epoch 40: 0.0221\n",
      "  Epoch 41: 0.0407\n",
      "  Epoch 42: 0.0216\n",
      "  Epoch 43: 0.0348\n",
      "  Epoch 44: 0.0438\n",
      "  Epoch 45: 0.0408\n",
      "  Epoch 46: 0.0418\n",
      "  Epoch 47: 0.0437\n",
      "  Epoch 48: 0.0301\n",
      "  Epoch 49: 0.0181\n",
      "  Epoch 50: 0.0298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Continue training the model with improvements from Document 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# --- Configuration ---\n",
    "class Config:\n",
    "    CHARS = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 .,-?!&\"\n",
    "    VOCAB_SIZE = len(CHARS) + 1\n",
    "    \n",
    "    # Training Parameters\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_EPOCHS = 50  # Additional epochs to train\n",
    "    LEARNING_RATE = 1e-3\n",
    "    \n",
    "    # Image Size\n",
    "    IMG_HEIGHT = 64\n",
    "    IMG_WIDTH = 256\n",
    "    HIDDEN_SIZE = 256\n",
    "    \n",
    "    # Directory Structure\n",
    "    BASE_DIR = os.getcwd()\n",
    "    CSV_DIR = os.path.join(BASE_DIR, \"CSV\")\n",
    "    TRAIN_IMG_DIR = os.path.join(BASE_DIR, \"train_v2/train\")\n",
    "    \n",
    "    # Annotation Files\n",
    "    TRAIN_CSV = os.path.join(CSV_DIR, \"written_name_train.csv\")\n",
    "    \n",
    "    # Model Paths\n",
    "    MINI_MODEL_PATH = os.path.join(BASE_DIR, \"htr_crnn_mini.pth\")\n",
    "\n",
    "\n",
    "def create_char_to_int_mapping(chars: str) -> Tuple[Dict[str, int], Dict[int, str]]:\n",
    "    char_to_int = {char: i + 1 for i, char in enumerate(chars)}\n",
    "    int_to_char = {i + 1: char for i, char in enumerate(chars)}\n",
    "    char_to_int['CTC_BLANK'] = 0\n",
    "    int_to_char[0] = ''\n",
    "    return char_to_int, int_to_char\n",
    "\n",
    "CHAR_TO_INT, INT_TO_CHAR = create_char_to_int_mapping(Config.CHARS)\n",
    "\n",
    "\n",
    "def load_all_annotations() -> Dict[str, Tuple[List[Tuple[str, str]], str]]:\n",
    "    csv_path = Config.TRAIN_CSV\n",
    "    img_dir = Config.TRAIN_IMG_DIR\n",
    "    \n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"❌ CSV not found at {csv_path}\")\n",
    "        return {}\n",
    "    \n",
    "    df = pd.read_csv(csv_path)\n",
    "    df.columns = ['image_filename', 'transcription_text']\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    df['exists'] = df['image_filename'].apply(lambda x: os.path.exists(os.path.join(img_dir, x)))\n",
    "    df = df[df['exists']].drop(columns=['exists'])\n",
    "\n",
    "    annotations = list(df[['image_filename', 'transcription_text']].itertuples(index=False, name=None))\n",
    "    print(f\"✅ Loaded {len(annotations)} valid records for training.\")\n",
    "    \n",
    "    return {'train': (annotations, img_dir)}\n",
    "\n",
    "\n",
    "class HTRDataset(Dataset):\n",
    "    def __init__(self, img_dir: str, annotations: List[Tuple[str, str]], transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.annotations = annotations\n",
    "        self.transform = transform\n",
    "        self.char_to_int = CHAR_TO_INT\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_filename, label_text = self.annotations[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_filename)\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception:\n",
    "            image = Image.new('RGB', (Config.IMG_WIDTH, Config.IMG_HEIGHT), color='black')\n",
    "            \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label_encoded = [self.char_to_int.get(char, 0) for char in label_text]\n",
    "        target = torch.tensor(label_encoded, dtype=torch.long)\n",
    "        target_len = torch.tensor(len(target), dtype=torch.long)\n",
    "        \n",
    "        return image, target, target_len\n",
    "\n",
    "\n",
    "def collate_fn(batch, model_cnn_output_width=None):\n",
    "    images, targets, target_lengths = zip(*batch)\n",
    "    images = torch.stack(images, 0)\n",
    "    \n",
    "    max_target_len = max(target_lengths)\n",
    "    padded_targets = torch.zeros((len(targets), max_target_len), dtype=torch.long)\n",
    "    for i, target in enumerate(targets):\n",
    "        padded_targets[i, :target.size(0)] = target\n",
    "    \n",
    "    target_lengths = torch.stack(target_lengths)\n",
    "    \n",
    "    # Use computed CNN output width if provided, else use approximation\n",
    "    if model_cnn_output_width is None:\n",
    "        model_cnn_output_width = Config.IMG_WIDTH // 4\n",
    "    \n",
    "    input_lengths = torch.full((len(batch),), model_cnn_output_width, dtype=torch.long)\n",
    "    \n",
    "    return images, padded_targets, input_lengths, target_lengths\n",
    "\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, img_height, vocab_size, hidden_size):\n",
    "        super(CRNN, self).__init__()\n",
    "        \n",
    "        # Improved CNN with deeper architecture and BatchNorm\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 1), (2, 1)),\n",
    "            nn.Conv2d(256, 512, 3, padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, None))\n",
    "        )\n",
    "        \n",
    "        # Map from CNN output (512 channels) to RNN input\n",
    "        self.map_to_rnn = nn.Linear(512, hidden_size)\n",
    "        \n",
    "        # Improved RNN with dropout and extra layer\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=3,\n",
    "            bidirectional=True,\n",
    "            dropout=0.3,\n",
    "            batch_first=False\n",
    "        )\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size * 2, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        cnn_out = self.cnn(x)\n",
    "        cnn_out = cnn_out.squeeze(2)\n",
    "        cnn_out = cnn_out.permute(0, 2, 1)\n",
    "        \n",
    "        rnn_input = self.map_to_rnn(cnn_out)\n",
    "        rnn_input = rnn_input.permute(1, 0, 2)\n",
    "        \n",
    "        rnn_out, _ = self.rnn(rnn_input)\n",
    "        output = self.linear(rnn_out)\n",
    "        output = nn.functional.log_softmax(output, dim=2)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def get_cnn_output_width(self, batch_size=1, device='cpu'):\n",
    "        \"\"\"Compute actual CNN output width dynamically\"\"\"\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(batch_size, 3, Config.IMG_HEIGHT, Config.IMG_WIDTH, device=device)\n",
    "            cnn_out = self.cnn(dummy_input)\n",
    "            return cnn_out.size(-1)\n",
    "\n",
    "\n",
    "def train_htr_model(data_loader, model, criterion, optimizer, device, scheduler=None):\n",
    "    \"\"\"Training with gradient clipping - returns lowest loss from epoch\"\"\"\n",
    "    model.train()\n",
    "    min_loss = float('inf')\n",
    "    \n",
    "    pbar = tqdm(data_loader, desc=\"Training\")\n",
    "    for images, targets, input_lengths, target_lengths in pbar:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = criterion(output, targets, input_lengths.to(device), target_lengths.to(device))\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping to prevent explosions\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_loss = loss.item()\n",
    "        min_loss = min(min_loss, batch_loss)\n",
    "        pbar.set_postfix({'batch_loss': f'{batch_loss:.4f}', 'min_loss': f'{min_loss:.4f}'})\n",
    "\n",
    "    return min_loss\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\\n\")\n",
    "    \n",
    "    # Check if model exists\n",
    "    if not os.path.exists(Config.MINI_MODEL_PATH):\n",
    "        print(f\"❌ No existing model found at {Config.MINI_MODEL_PATH}\")\n",
    "        print(\"Please train the model first using the training script.\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Image transformations with augmentation and normalization\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((Config.IMG_HEIGHT, Config.IMG_WIDTH)),\n",
    "        transforms.RandomAffine(degrees=3, translate=(0.02, 0.02), shear=2),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    \n",
    "    # Load data\n",
    "    all_data = load_all_annotations()\n",
    "    \n",
    "    if 'train' not in all_data:\n",
    "        print(\"❌ Training data not found!\")\n",
    "        exit(1)\n",
    "    \n",
    "    train_annotations, train_img_dir = all_data['train']\n",
    "    train_dataset = HTRDataset(img_dir=train_img_dir, annotations=train_annotations, transform=train_transform)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CONTINUE TRAINING FROM SAVED MODEL (IMPROVED)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    mini_size = 5000\n",
    "    mini_indices = torch.randperm(len(train_dataset))[:mini_size].tolist()\n",
    "    mini_dataset = Subset(train_dataset, mini_indices)\n",
    "    \n",
    "    mini_loader = DataLoader(\n",
    "        mini_dataset,\n",
    "        batch_size=Config.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    print(f\"Mini dataset size: {len(mini_dataset)} samples\")\n",
    "    \n",
    "    # Load existing model\n",
    "    print(f\"\\nLoading existing model from: {Config.MINI_MODEL_PATH}\")\n",
    "    model_mini = CRNN(Config.IMG_HEIGHT, Config.VOCAB_SIZE, Config.HIDDEN_SIZE).to(device)\n",
    "    model_mini.load_state_dict(torch.load(Config.MINI_MODEL_PATH, map_location=device))\n",
    "    print(f\"✅ Model loaded successfully!\")\n",
    "    \n",
    "    # Compute actual CNN output width for accurate CTC loss\n",
    "    cnn_output_width = model_mini.get_cnn_output_width(batch_size=1, device=device)\n",
    "    print(f\"CNN output width: {cnn_output_width}\")\n",
    "    \n",
    "    # Update collate function to use actual CNN output width\n",
    "    def collate_fn_with_width(batch):\n",
    "        return collate_fn(batch, model_cnn_output_width=cnn_output_width)\n",
    "    \n",
    "    mini_loader = DataLoader(\n",
    "        mini_dataset,\n",
    "        batch_size=Config.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn_with_width\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nContinuing training for {Config.NUM_EPOCHS} more epochs...\\n\")\n",
    "    \n",
    "    criterion = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
    "    optimizer_mini = optim.Adam(model_mini.parameters(), lr=Config.LEARNING_RATE)\n",
    "    \n",
    "    # Add learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer_mini, mode='min', factor=0.5, patience=3, verbose=True\n",
    "    )\n",
    "    \n",
    "    best_loss_mini = float('inf')\n",
    "    loss_history = []\n",
    "    \n",
    "    for epoch in range(1, Config.NUM_EPOCHS + 1):\n",
    "        print(f\"Epoch {epoch}/{Config.NUM_EPOCHS}\")\n",
    "        min_loss = train_htr_model(mini_loader, model_mini, criterion, optimizer_mini, device, scheduler)\n",
    "        print(f\"Minimum Loss: {min_loss:.4f}\\n\")\n",
    "        \n",
    "        loss_history.append(min_loss)\n",
    "        \n",
    "        # Step learning rate based on loss plateau\n",
    "        scheduler.step(min_loss)\n",
    "        \n",
    "        if min_loss < best_loss_mini:\n",
    "            best_loss_mini = min_loss\n",
    "            torch.save(model_mini.state_dict(), Config.MINI_MODEL_PATH)\n",
    "            print(f\"✅ Best model saved! Lowest loss: {best_loss_mini:.4f}\\n\")\n",
    "        else:\n",
    "            print(f\"Loss did not improve. Best so far: {best_loss_mini:.4f}\\n\")\n",
    "    \n",
    "    print(f\"\\n✅ Continued training complete! Best loss achieved: {best_loss_mini:.4f}\")\n",
    "    print(f\"Model saved to: {Config.MINI_MODEL_PATH}\")\n",
    "    print(f\"\\nLoss history by epoch:\")\n",
    "    for epoch, loss in enumerate(loss_history, 1):\n",
    "        print(f\"  Epoch {epoch}: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7df62f4c-396b-42a8-a7ff-992807c823b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CRNN MODEL - TESTING/INFERENCE\n",
      "============================================================\n",
      "\n",
      "Using device: cpu\n",
      "\n",
      "Loading CRNN model...\n",
      "✅ Model loaded successfully\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmed\\AppData\\Local\\Temp\\ipykernel_10224\\2990595633.py:201: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 41370 images. Starting predictions...\n",
      "\n",
      "✅ [1/41370] TEST_0001.jpg\n",
      "   Predicted: 'KEVIN' (Confidence: 0.9917)\n",
      "\n",
      "✅ [2/41370] TEST_0002.jpg\n",
      "   Predicted: 'COIINE' (Confidence: 0.6787)\n",
      "\n",
      "✅ [3/41370] TEST_0003.jpg\n",
      "   Predicted: 'LENA' (Confidence: 0.9980)\n",
      "\n",
      "✅ [4/41370] TEST_0004.jpg\n",
      "   Predicted: 'JULES' (Confidence: 0.9995)\n",
      "\n",
      "✅ [5/41370] TEST_0005.jpg\n",
      "   Predicted: 'CHERDIN' (Confidence: 0.9070)\n",
      "\n",
      "✅ [6/41370] TEST_0006.jpg\n",
      "   Predicted: 'MARTIN' (Confidence: 0.9986)\n",
      "\n",
      "✅ [7/41370] TEST_0007.jpg\n",
      "   Predicted: 'VALENTINE' (Confidence: 0.9994)\n",
      "\n",
      "✅ [8/41370] TEST_0008.jpg\n",
      "   Predicted: 'CORAS' (Confidence: 0.8579)\n",
      "\n",
      "✅ [9/41370] TEST_0009.jpg\n",
      "   Predicted: 'THIBAULT' (Confidence: 0.9941)\n",
      "\n",
      "✅ [10/41370] TEST_0010.jpg\n",
      "   Predicted: 'AERBI' (Confidence: 0.8235)\n",
      "\n",
      "✅ [11/41370] TEST_0011.jpg\n",
      "   Predicted: 'BORT-MARFE' (Confidence: 0.6474)\n",
      "\n",
      "✅ [12/41370] TEST_0012.jpg\n",
      "   Predicted: 'MANEHINIRAN' (Confidence: 0.8750)\n",
      "\n",
      "✅ [13/41370] TEST_0013.jpg\n",
      "   Predicted: 'FANIESSRSEA' (Confidence: 0.7266)\n",
      "\n",
      "✅ [14/41370] TEST_0014.jpg\n",
      "   Predicted: 'JEANNE' (Confidence: 0.9995)\n",
      "\n",
      "✅ [15/41370] TEST_0015.jpg\n",
      "   Predicted: 'DEBOAAH' (Confidence: 0.7405)\n",
      "\n",
      "✅ [16/41370] TEST_0016.jpg\n",
      "   Predicted: 'BADUES' (Confidence: 0.8229)\n",
      "\n",
      "✅ [17/41370] TEST_0017.jpg\n",
      "   Predicted: 'JAAC' (Confidence: 0.7917)\n",
      "\n",
      "✅ [18/41370] TEST_0018.jpg\n",
      "   Predicted: 'CONNIIZPCERROT' (Confidence: 0.8046)\n",
      "\n",
      "✅ [19/41370] TEST_0019.jpg\n",
      "   Predicted: 'CLEMENT' (Confidence: 0.9995)\n",
      "\n",
      "✅ [20/41370] TEST_0020.jpg\n",
      "   Predicted: 'AELIS' (Confidence: 0.9401)\n",
      "\n",
      "✅ [21/41370] TEST_0021.jpg\n",
      "   Predicted: 'MANTIR-   CAIM' (Confidence: 0.7550)\n",
      "\n",
      "✅ [22/41370] TEST_0022.jpg\n",
      "   Predicted: 'FOUANEL' (Confidence: 0.9048)\n",
      "\n",
      "✅ [23/41370] TEST_0023.jpg\n",
      "   Predicted: 'DIETITSSIILLFA' (Confidence: 0.5815)\n",
      "\n",
      "✅ [24/41370] TEST_0024.jpg\n",
      "   Predicted: 'BARAOT' (Confidence: 0.9740)\n",
      "\n",
      "✅ [25/41370] TEST_0025.jpg\n",
      "   Predicted: 'DUVAL' (Confidence: 0.8714)\n",
      "\n",
      "✅ [26/41370] TEST_0026.jpg\n",
      "   Predicted: 'ANTONY' (Confidence: 0.9853)\n",
      "\n",
      "✅ [27/41370] TEST_0027.jpg\n",
      "   Predicted: 'LISM' (Confidence: 0.8772)\n",
      "\n",
      "✅ [28/41370] TEST_0028.jpg\n",
      "   Predicted: 'CELIA' (Confidence: 0.9819)\n",
      "\n",
      "✅ [29/41370] TEST_0029.jpg\n",
      "   Predicted: 'GABOIAT' (Confidence: 0.8012)\n",
      "\n",
      "✅ [30/41370] TEST_0030.jpg\n",
      "   Predicted: 'HUGO' (Confidence: 0.9996)\n",
      "\n",
      "✅ [31/41370] TEST_0031.jpg\n",
      "   Predicted: 'HUGO' (Confidence: 0.9971)\n",
      "\n",
      "✅ [32/41370] TEST_0032.jpg\n",
      "   Predicted: 'ELOVEN' (Confidence: 0.9528)\n",
      "\n",
      "✅ [33/41370] TEST_0033.jpg\n",
      "   Predicted: 'SCHAEMMCER' (Confidence: 0.8498)\n",
      "\n",
      "✅ [34/41370] TEST_0034.jpg\n",
      "   Predicted: 'MORAAY' (Confidence: 0.7576)\n",
      "\n",
      "✅ [35/41370] TEST_0035.jpg\n",
      "   Predicted: 'STEEVKERFTE' (Confidence: 0.7685)\n",
      "\n",
      "✅ [36/41370] TEST_0036.jpg\n",
      "   Predicted: 'DIFFONG' (Confidence: 0.8124)\n",
      "\n",
      "✅ [37/41370] TEST_0037.jpg\n",
      "   Predicted: 'DUTREY' (Confidence: 0.8048)\n",
      "\n",
      "✅ [38/41370] TEST_0038.jpg\n",
      "   Predicted: 'RONAN' (Confidence: 0.9714)\n",
      "\n",
      "✅ [39/41370] TEST_0039.jpg\n",
      "   Predicted: 'MILLE' (Confidence: 0.9485)\n",
      "\n",
      "✅ [40/41370] TEST_0040.jpg\n",
      "   Predicted: 'CAROLINE' (Confidence: 0.9982)\n",
      "\n",
      "✅ [41/41370] TEST_0041.jpg\n",
      "   Predicted: 'PAUL' (Confidence: 0.9994)\n",
      "\n",
      "✅ [42/41370] TEST_0042.jpg\n",
      "   Predicted: 'BELHARA' (Confidence: 0.9311)\n",
      "\n",
      "✅ [43/41370] TEST_0043.jpg\n",
      "   Predicted: 'DIOOS' (Confidence: 0.8560)\n",
      "\n",
      "✅ [44/41370] TEST_0044.jpg\n",
      "   Predicted: 'JENEONE' (Confidence: 0.8889)\n",
      "\n",
      "✅ [45/41370] TEST_0045.jpg\n",
      "   Predicted: 'OREVIX' (Confidence: 0.7405)\n",
      "\n",
      "✅ [46/41370] TEST_0046.jpg\n",
      "   Predicted: 'CLOUIS' (Confidence: 0.8798)\n",
      "\n",
      "✅ [47/41370] TEST_0047.jpg\n",
      "   Predicted: 'JORIS' (Confidence: 0.9717)\n",
      "\n",
      "✅ [48/41370] TEST_0048.jpg\n",
      "   Predicted: 'MARRINI' (Confidence: 0.9860)\n",
      "\n",
      "✅ [49/41370] TEST_0049.jpg\n",
      "   Predicted: 'CLAIRE' (Confidence: 0.9988)\n",
      "\n",
      "✅ [50/41370] TEST_0050.jpg\n",
      "   Predicted: 'KAFFORT' (Confidence: 0.8485)\n",
      "\n",
      "✅ [51/41370] TEST_0051.jpg\n",
      "   Predicted: 'LEMDADIS' (Confidence: 0.7602)\n",
      "\n",
      "✅ [52/41370] TEST_0052.jpg\n",
      "   Predicted: 'BESUOSSHER' (Confidence: 0.8511)\n",
      "\n",
      "✅ [53/41370] TEST_0053.jpg\n",
      "   Predicted: 'ALEXANDRE' (Confidence: 0.9996)\n",
      "\n",
      "✅ [54/41370] TEST_0054.jpg\n",
      "   Predicted: 'RAPHEAEL' (Confidence: 0.9788)\n",
      "\n",
      "✅ [55/41370] TEST_0055.jpg\n",
      "   Predicted: 'AIRERDINE' (Confidence: 0.9144)\n",
      "\n",
      "✅ [56/41370] TEST_0056.jpg\n",
      "   Predicted: 'ANNT' (Confidence: 0.7422)\n",
      "\n",
      "✅ [57/41370] TEST_0057.jpg\n",
      "   Predicted: 'GOURAUD' (Confidence: 0.9901)\n",
      "\n",
      "✅ [58/41370] TEST_0058.jpg\n",
      "   Predicted: 'RONAN' (Confidence: 0.8940)\n",
      "\n",
      "✅ [59/41370] TEST_0059.jpg\n",
      "   Predicted: 'LEREUHANN' (Confidence: 0.8170)\n",
      "\n",
      "✅ [60/41370] TEST_0060.jpg\n",
      "   Predicted: 'MOBIER' (Confidence: 0.9158)\n",
      "\n",
      "✅ [61/41370] TEST_0061.jpg\n",
      "   Predicted: 'MARDY' (Confidence: 0.9898)\n",
      "\n",
      "✅ [62/41370] TEST_0062.jpg\n",
      "   Predicted: 'ALEXANDRE' (Confidence: 0.9996)\n",
      "\n",
      "✅ [63/41370] TEST_0063.jpg\n",
      "   Predicted: 'BOUCANSER' (Confidence: 0.8930)\n",
      "\n",
      "✅ [64/41370] TEST_0064.jpg\n",
      "   Predicted: 'HARMANE' (Confidence: 0.8288)\n",
      "\n",
      "✅ [65/41370] TEST_0065.jpg\n",
      "   Predicted: 'PARIS' (Confidence: 0.9214)\n",
      "\n",
      "✅ [66/41370] TEST_0066.jpg\n",
      "   Predicted: 'KELANIE' (Confidence: 0.9366)\n",
      "\n",
      "✅ [67/41370] TEST_0067.jpg\n",
      "   Predicted: 'NOUNIE' (Confidence: 0.7598)\n",
      "\n",
      "✅ [68/41370] TEST_0068.jpg\n",
      "   Predicted: 'LENAIT' (Confidence: 0.9427)\n",
      "\n",
      "✅ [69/41370] TEST_0069.jpg\n",
      "   Predicted: 'DARENT' (Confidence: 0.8685)\n",
      "\n",
      "✅ [70/41370] TEST_0070.jpg\n",
      "   Predicted: 'DESCHERES' (Confidence: 0.8964)\n",
      "\n",
      "✅ [71/41370] TEST_0071.jpg\n",
      "   Predicted: 'NAUAULT' (Confidence: 0.8194)\n",
      "\n",
      "✅ [72/41370] TEST_0072.jpg\n",
      "   Predicted: 'DASOAE' (Confidence: 0.8652)\n",
      "\n",
      "✅ [73/41370] TEST_0073.jpg\n",
      "   Predicted: 'QUENAINE' (Confidence: 0.8781)\n",
      "\n",
      "✅ [74/41370] TEST_0074.jpg\n",
      "   Predicted: 'ATOUL' (Confidence: 0.9639)\n",
      "\n",
      "✅ [75/41370] TEST_0075.jpg\n",
      "   Predicted: 'MAREN' (Confidence: 0.9495)\n",
      "\n",
      "✅ [76/41370] TEST_0076.jpg\n",
      "   Predicted: 'AREXIR' (Confidence: 0.7994)\n",
      "\n",
      "✅ [77/41370] TEST_0077.jpg\n",
      "   Predicted: 'ESGAN' (Confidence: 0.8272)\n",
      "\n",
      "✅ [78/41370] TEST_0078.jpg\n",
      "   Predicted: 'BEAUCHET' (Confidence: 0.9219)\n",
      "\n",
      "✅ [79/41370] TEST_0079.jpg\n",
      "   Predicted: 'KEVIN' (Confidence: 0.9756)\n",
      "\n",
      "✅ [80/41370] TEST_0080.jpg\n",
      "   Predicted: 'CANAITI' (Confidence: 0.8335)\n",
      "\n",
      "✅ [81/41370] TEST_0081.jpg\n",
      "   Predicted: 'ALESSI' (Confidence: 0.9090)\n",
      "\n",
      "✅ [82/41370] TEST_0082.jpg\n",
      "   Predicted: 'CHARLES' (Confidence: 0.9951)\n",
      "\n",
      "✅ [83/41370] TEST_0083.jpg\n",
      "   Predicted: 'ELIA' (Confidence: 0.9992)\n",
      "\n",
      "✅ [84/41370] TEST_0084.jpg\n",
      "   Predicted: 'ALEXANDRE' (Confidence: 0.9994)\n",
      "\n",
      "✅ [85/41370] TEST_0085.jpg\n",
      "   Predicted: 'ROYTIEOUTLLLIER' (Confidence: 0.8296)\n",
      "\n",
      "✅ [86/41370] TEST_0086.jpg\n",
      "   Predicted: 'LEYLI' (Confidence: 0.8855)\n",
      "\n",
      "✅ [87/41370] TEST_0087.jpg\n",
      "   Predicted: 'YANAELAN' (Confidence: 0.8773)\n",
      "\n",
      "✅ [88/41370] TEST_0088.jpg\n",
      "   Predicted: 'MADRIEN' (Confidence: 0.9543)\n",
      "\n",
      "✅ [89/41370] TEST_0089.jpg\n",
      "   Predicted: 'AIDIY' (Confidence: 0.6778)\n",
      "\n",
      "✅ [90/41370] TEST_0090.jpg\n",
      "   Predicted: 'DEGA' (Confidence: 0.8315)\n",
      "\n",
      "✅ [91/41370] TEST_0091.jpg\n",
      "   Predicted: 'CONTL-RRIIPRLAS' (Confidence: 0.7844)\n",
      "\n",
      "✅ [92/41370] TEST_0092.jpg\n",
      "   Predicted: 'CHANCELOUX' (Confidence: 0.8125)\n",
      "\n",
      "✅ [93/41370] TEST_0093.jpg\n",
      "   Predicted: 'FLOREIN' (Confidence: 0.9885)\n",
      "\n",
      "✅ [94/41370] TEST_0094.jpg\n",
      "   Predicted: 'TASKIN' (Confidence: 0.8284)\n",
      "\n",
      "✅ [95/41370] TEST_0095.jpg\n",
      "   Predicted: 'BARRAUD' (Confidence: 0.8787)\n",
      "\n",
      "✅ [96/41370] TEST_0096.jpg\n",
      "   Predicted: 'VENJYRA' (Confidence: 0.8641)\n",
      "\n",
      "✅ [97/41370] TEST_0097.jpg\n",
      "   Predicted: 'NOUR' (Confidence: 0.9392)\n",
      "\n",
      "✅ [98/41370] TEST_0098.jpg\n",
      "   Predicted: 'LISE' (Confidence: 0.9668)\n",
      "\n",
      "✅ [99/41370] TEST_0099.jpg\n",
      "   Predicted: 'JUSTINE' (Confidence: 0.9936)\n",
      "\n",
      "✅ [100/41370] TEST_0100.jpg\n",
      "   Predicted: 'LEO' (Confidence: 0.9999)\n",
      "\n",
      "✅ [101/41370] TEST_0101.jpg\n",
      "   Predicted: 'LADU' (Confidence: 0.8011)\n",
      "\n",
      "✅ [102/41370] TEST_0102.jpg\n",
      "   Predicted: 'HANMAHAR' (Confidence: 0.7859)\n",
      "\n",
      "✅ [103/41370] TEST_0103.jpg\n",
      "   Predicted: 'GAN' (Confidence: 0.7607)\n",
      "\n",
      "✅ [104/41370] TEST_0104.jpg\n",
      "   Predicted: 'AUDUOT' (Confidence: 0.8273)\n",
      "\n",
      "✅ [105/41370] TEST_0105.jpg\n",
      "   Predicted: 'EMMA' (Confidence: 0.8864)\n",
      "\n",
      "✅ [106/41370] TEST_0106.jpg\n",
      "   Predicted: 'VERREAUX' (Confidence: 0.9228)\n",
      "\n",
      "✅ [107/41370] TEST_0107.jpg\n",
      "   Predicted: 'ANBET' (Confidence: 0.9583)\n",
      "\n",
      "✅ [108/41370] TEST_0108.jpg\n",
      "   Predicted: 'TRISTAN' (Confidence: 0.9849)\n",
      "\n",
      "✅ [109/41370] TEST_0109.jpg\n",
      "   Predicted: 'FONTENELLE' (Confidence: 0.9052)\n",
      "\n",
      "✅ [110/41370] TEST_0110.jpg\n",
      "   Predicted: 'GUILLAUME' (Confidence: 0.9587)\n",
      "\n",
      "✅ [111/41370] TEST_0111.jpg\n",
      "   Predicted: 'THALIA' (Confidence: 0.9745)\n",
      "\n",
      "✅ [112/41370] TEST_0112.jpg\n",
      "   Predicted: 'CECESTINE' (Confidence: 0.9054)\n",
      "\n",
      "✅ [113/41370] TEST_0113.jpg\n",
      "   Predicted: 'DELEMRDUE' (Confidence: 0.8901)\n",
      "\n",
      "✅ [114/41370] TEST_0114.jpg\n",
      "   Predicted: 'GAUH' (Confidence: 0.8107)\n",
      "\n",
      "✅ [115/41370] TEST_0115.jpg\n",
      "   Predicted: 'SIMON' (Confidence: 0.9326)\n",
      "\n",
      "✅ [116/41370] TEST_0116.jpg\n",
      "   Predicted: 'MATTHIEU' (Confidence: 0.8443)\n",
      "\n",
      "✅ [117/41370] TEST_0117.jpg\n",
      "   Predicted: 'BEAUVILCE' (Confidence: 0.9108)\n",
      "\n",
      "✅ [118/41370] TEST_0118.jpg\n",
      "   Predicted: 'VALERE' (Confidence: 0.9965)\n",
      "\n",
      "✅ [119/41370] TEST_0119.jpg\n",
      "   Predicted: 'BARY' (Confidence: 0.9098)\n",
      "\n",
      "✅ [120/41370] TEST_0120.jpg\n",
      "   Predicted: 'CHARAREGHI' (Confidence: 0.7910)\n",
      "\n",
      "✅ [121/41370] TEST_0121.jpg\n",
      "   Predicted: 'PIMOULT' (Confidence: 0.8904)\n",
      "\n",
      "✅ [122/41370] TEST_0122.jpg\n",
      "   Predicted: 'DINEOIN' (Confidence: 0.8274)\n",
      "\n",
      "✅ [123/41370] TEST_0123.jpg\n",
      "   Predicted: 'OSCAN' (Confidence: 0.8642)\n",
      "\n",
      "✅ [124/41370] TEST_0124.jpg\n",
      "   Predicted: 'BELBARD' (Confidence: 0.8115)\n",
      "\n",
      "✅ [125/41370] TEST_0125.jpg\n",
      "   Predicted: 'ANBRE' (Confidence: 0.9114)\n",
      "\n",
      "✅ [126/41370] TEST_0126.jpg\n",
      "   Predicted: 'SERADI' (Confidence: 0.8701)\n",
      "\n",
      "✅ [127/41370] TEST_0127.jpg\n",
      "   Predicted: 'MARTIN' (Confidence: 0.9986)\n",
      "\n",
      "✅ [128/41370] TEST_0128.jpg\n",
      "   Predicted: 'RABNE' (Confidence: 0.8523)\n",
      "\n",
      "✅ [129/41370] TEST_0129.jpg\n",
      "   Predicted: 'MENEUX' (Confidence: 0.8715)\n",
      "\n",
      "✅ [130/41370] TEST_0130.jpg\n",
      "   Predicted: 'CORALINE' (Confidence: 0.9904)\n",
      "\n",
      "✅ [131/41370] TEST_0131.jpg\n",
      "   Predicted: 'VINCENT' (Confidence: 0.9673)\n",
      "\n",
      "✅ [132/41370] TEST_0132.jpg\n",
      "   Predicted: 'SARA' (Confidence: 0.9605)\n",
      "\n",
      "✅ [133/41370] TEST_0133.jpg\n",
      "   Predicted: 'DUPLAIA' (Confidence: 0.9800)\n",
      "\n",
      "✅ [134/41370] TEST_0134.jpg\n",
      "   Predicted: 'JEANNE' (Confidence: 0.9989)\n",
      "\n",
      "✅ [135/41370] TEST_0135.jpg\n",
      "   Predicted: 'ARTHYE' (Confidence: 0.8692)\n",
      "\n",
      "✅ [136/41370] TEST_0136.jpg\n",
      "   Predicted: 'MARTINET' (Confidence: 0.9939)\n",
      "\n",
      "✅ [137/41370] TEST_0137.jpg\n",
      "   Predicted: 'LERIN' (Confidence: 0.9994)\n",
      "\n",
      "✅ [138/41370] TEST_0138.jpg\n",
      "   Predicted: 'PHILIPPE' (Confidence: 0.9427)\n",
      "\n",
      "✅ [139/41370] TEST_0139.jpg\n",
      "   Predicted: 'RENNAUD' (Confidence: 0.8747)\n",
      "\n",
      "✅ [140/41370] TEST_0140.jpg\n",
      "   Predicted: 'SENEZET' (Confidence: 0.8251)\n",
      "\n",
      "✅ [141/41370] TEST_0141.jpg\n",
      "   Predicted: 'EMMA' (Confidence: 0.9979)\n",
      "\n",
      "✅ [142/41370] TEST_0142.jpg\n",
      "   Predicted: 'TONY' (Confidence: 0.9553)\n",
      "\n",
      "✅ [143/41370] TEST_0143.jpg\n",
      "   Predicted: 'ENZO' (Confidence: 0.9349)\n",
      "\n",
      "✅ [144/41370] TEST_0144.jpg\n",
      "   Predicted: 'LEVELE EERBL' (Confidence: 0.8260)\n",
      "\n",
      "✅ [145/41370] TEST_0145.jpg\n",
      "   Predicted: 'WOEL' (Confidence: 0.8688)\n",
      "\n",
      "✅ [146/41370] TEST_0146.jpg\n",
      "   Predicted: 'LAGROIX' (Confidence: 0.9064)\n",
      "\n",
      "✅ [147/41370] TEST_0147.jpg\n",
      "   Predicted: 'LANDCAN' (Confidence: 0.8716)\n",
      "\n",
      "✅ [148/41370] TEST_0148.jpg\n",
      "   Predicted: 'AMIEIE' (Confidence: 0.8789)\n",
      "\n",
      "✅ [149/41370] TEST_0149.jpg\n",
      "   Predicted: 'FRANCOIS' (Confidence: 0.9881)\n",
      "\n",
      "✅ [150/41370] TEST_0150.jpg\n",
      "   Predicted: 'MAUY' (Confidence: 0.6795)\n",
      "\n",
      "✅ [151/41370] TEST_0151.jpg\n",
      "   Predicted: 'VICTR-AEMMARIE' (Confidence: 0.8349)\n",
      "\n",
      "✅ [152/41370] TEST_0152.jpg\n",
      "   Predicted: 'BOURAUD' (Confidence: 0.9590)\n",
      "\n",
      "✅ [153/41370] TEST_0153.jpg\n",
      "   Predicted: 'SIROT' (Confidence: 0.9911)\n",
      "\n",
      "✅ [154/41370] TEST_0154.jpg\n",
      "   Predicted: 'SORADIN' (Confidence: 0.8425)\n",
      "\n",
      "✅ [155/41370] TEST_0155.jpg\n",
      "   Predicted: 'BRETON' (Confidence: 0.9957)\n",
      "\n",
      "✅ [156/41370] TEST_0156.jpg\n",
      "   Predicted: 'NOLHEM' (Confidence: 0.8960)\n",
      "\n",
      "✅ [157/41370] TEST_0157.jpg\n",
      "   Predicted: 'MEROUANE' (Confidence: 0.9435)\n",
      "\n",
      "✅ [158/41370] TEST_0158.jpg\n",
      "   Predicted: 'BIMIHONSKI' (Confidence: 0.8032)\n",
      "\n",
      "✅ [159/41370] TEST_0159.jpg\n",
      "   Predicted: 'REYNGRRCH' (Confidence: 0.8658)\n",
      "\n",
      "✅ [160/41370] TEST_0160.jpg\n",
      "   Predicted: 'NOLAN' (Confidence: 0.9168)\n",
      "\n",
      "✅ [161/41370] TEST_0161.jpg\n",
      "   Predicted: 'AYHOY' (Confidence: 0.6849)\n",
      "\n",
      "✅ [162/41370] TEST_0162.jpg\n",
      "   Predicted: 'EMMA' (Confidence: 0.9965)\n",
      "\n",
      "✅ [163/41370] TEST_0163.jpg\n",
      "   Predicted: 'LUCIE' (Confidence: 1.0000)\n",
      "\n",
      "✅ [164/41370] TEST_0164.jpg\n",
      "   Predicted: 'CIMINO' (Confidence: 0.9069)\n",
      "\n",
      "✅ [165/41370] TEST_0165.jpg\n",
      "   Predicted: 'NICOLAS' (Confidence: 0.9894)\n",
      "\n",
      "✅ [166/41370] TEST_0166.jpg\n",
      "   Predicted: 'JIANA' (Confidence: 0.8876)\n",
      "\n",
      "✅ [167/41370] TEST_0167.jpg\n",
      "   Predicted: 'CAMILLE' (Confidence: 0.9963)\n",
      "\n",
      "✅ [168/41370] TEST_0168.jpg\n",
      "   Predicted: 'SAPOAR' (Confidence: 0.8153)\n",
      "\n",
      "✅ [169/41370] TEST_0169.jpg\n",
      "   Predicted: 'CANITGNY' (Confidence: 0.7942)\n",
      "\n",
      "✅ [170/41370] TEST_0170.jpg\n",
      "   Predicted: 'INES' (Confidence: 0.9971)\n",
      "\n",
      "✅ [171/41370] TEST_0171.jpg\n",
      "   Predicted: 'AURDRE' (Confidence: 0.9497)\n",
      "\n",
      "✅ [172/41370] TEST_0172.jpg\n",
      "   Predicted: 'LAVIGNE' (Confidence: 0.9412)\n",
      "\n",
      "✅ [173/41370] TEST_0173.jpg\n",
      "   Predicted: 'BADIN' (Confidence: 0.8371)\n",
      "\n",
      "✅ [174/41370] TEST_0174.jpg\n",
      "   Predicted: 'DUBBEHI' (Confidence: 0.8403)\n",
      "\n",
      "✅ [175/41370] TEST_0175.jpg\n",
      "   Predicted: 'RAROUJ' (Confidence: 0.7426)\n",
      "\n",
      "✅ [176/41370] TEST_0176.jpg\n",
      "   Predicted: 'EWEN' (Confidence: 0.9881)\n",
      "\n",
      "✅ [177/41370] TEST_0177.jpg\n",
      "   Predicted: 'NOA' (Confidence: 0.9025)\n",
      "\n",
      "✅ [178/41370] TEST_0178.jpg\n",
      "   Predicted: 'KYLIAN' (Confidence: 0.8191)\n",
      "\n",
      "✅ [179/41370] TEST_0179.jpg\n",
      "   Predicted: 'HUGO' (Confidence: 0.9922)\n",
      "\n",
      "✅ [180/41370] TEST_0180.jpg\n",
      "   Predicted: 'LUDOVICA' (Confidence: 0.9132)\n",
      "\n",
      "✅ [181/41370] TEST_0181.jpg\n",
      "   Predicted: 'METER' (Confidence: 0.8724)\n",
      "\n",
      "✅ [182/41370] TEST_0182.jpg\n",
      "   Predicted: 'AMATINEN' (Confidence: 0.8712)\n",
      "\n",
      "✅ [183/41370] TEST_0183.jpg\n",
      "   Predicted: 'SAUEAIA' (Confidence: 0.8865)\n",
      "\n",
      "✅ [184/41370] TEST_0184.jpg\n",
      "   Predicted: 'NAE' (Confidence: 0.6541)\n",
      "\n",
      "✅ [185/41370] TEST_0185.jpg\n",
      "   Predicted: 'VANUENEE-SOONE' (Confidence: 0.6719)\n",
      "\n",
      "✅ [186/41370] TEST_0186.jpg\n",
      "   Predicted: 'MARIN' (Confidence: 0.8700)\n",
      "\n",
      "✅ [187/41370] TEST_0187.jpg\n",
      "   Predicted: 'CHOUCET' (Confidence: 0.9140)\n",
      "\n",
      "✅ [188/41370] TEST_0188.jpg\n",
      "   Predicted: 'EMPIE' (Confidence: 0.5516)\n",
      "\n",
      "✅ [189/41370] TEST_0189.jpg\n",
      "   Predicted: 'MOURIM' (Confidence: 0.8330)\n",
      "\n",
      "✅ [190/41370] TEST_0190.jpg\n",
      "   Predicted: 'SERATE' (Confidence: 0.8605)\n",
      "\n",
      "✅ [191/41370] TEST_0191.jpg\n",
      "   Predicted: 'MATHOB' (Confidence: 0.8468)\n",
      "\n",
      "✅ [192/41370] TEST_0192.jpg\n",
      "   Predicted: 'CLEMENCE' (Confidence: 0.9988)\n",
      "\n",
      "✅ [193/41370] TEST_0193.jpg\n",
      "   Predicted: 'LEA' (Confidence: 0.9999)\n",
      "\n",
      "✅ [194/41370] TEST_0194.jpg\n",
      "   Predicted: 'DONCET' (Confidence: 0.9387)\n",
      "\n",
      "✅ [195/41370] TEST_0195.jpg\n",
      "   Predicted: 'BOUCHART' (Confidence: 0.9914)\n",
      "\n",
      "✅ [196/41370] TEST_0196.jpg\n",
      "   Predicted: 'BO' (Confidence: 0.8193)\n",
      "\n",
      "✅ [197/41370] TEST_0197.jpg\n",
      "   Predicted: 'EVA' (Confidence: 0.9942)\n",
      "\n",
      "✅ [198/41370] TEST_0198.jpg\n",
      "   Predicted: 'LEMIHARIE' (Confidence: 0.7841)\n",
      "\n",
      "✅ [199/41370] TEST_0199.jpg\n",
      "   Predicted: 'CLEERNA' (Confidence: 0.8742)\n",
      "\n",
      "✅ [200/41370] TEST_0200.jpg\n",
      "   Predicted: 'ROMORD' (Confidence: 0.8940)\n",
      "\n",
      "✅ [201/41370] TEST_0201.jpg\n",
      "   Predicted: 'VERGET' (Confidence: 0.9172)\n",
      "\n",
      "✅ [202/41370] TEST_0202.jpg\n",
      "   Predicted: 'PUMR' (Confidence: 0.7725)\n",
      "\n",
      "✅ [203/41370] TEST_0203.jpg\n",
      "   Predicted: 'MEIIA' (Confidence: 0.7880)\n",
      "\n",
      "✅ [204/41370] TEST_0204.jpg\n",
      "   Predicted: 'CHAMAALLE' (Confidence: 0.9436)\n",
      "\n",
      "✅ [205/41370] TEST_0205.jpg\n",
      "   Predicted: 'LAUG-LMA-AIE' (Confidence: 0.7978)\n",
      "\n",
      "✅ [206/41370] TEST_0206.jpg\n",
      "   Predicted: 'VINIE' (Confidence: 0.8549)\n",
      "\n",
      "✅ [207/41370] TEST_0207.jpg\n",
      "   Predicted: 'CHIEU' (Confidence: 0.8760)\n",
      "\n",
      "✅ [208/41370] TEST_0208.jpg\n",
      "   Predicted: 'LEDUE' (Confidence: 0.7741)\n",
      "\n",
      "✅ [209/41370] TEST_0209.jpg\n",
      "   Predicted: 'ROSON' (Confidence: 0.9642)\n",
      "\n",
      "✅ [210/41370] TEST_0210.jpg\n",
      "   Predicted: 'ERWAN' (Confidence: 0.9983)\n",
      "\n",
      "✅ [211/41370] TEST_0211.jpg\n",
      "   Predicted: 'ROMAIN' (Confidence: 0.9714)\n",
      "\n",
      "✅ [212/41370] TEST_0212.jpg\n",
      "   Predicted: 'AONY' (Confidence: 0.9598)\n",
      "\n",
      "✅ [213/41370] TEST_0213.jpg\n",
      "   Predicted: 'NICOLAS' (Confidence: 0.9121)\n",
      "\n",
      "✅ [214/41370] TEST_0214.jpg\n",
      "   Predicted: 'JULIETTE' (Confidence: 0.9992)\n",
      "\n",
      "✅ [215/41370] TEST_0215.jpg\n",
      "   Predicted: 'CHLOE' (Confidence: 0.9893)\n",
      "\n",
      "✅ [216/41370] TEST_0216.jpg\n",
      "   Predicted: 'AISSON' (Confidence: 0.9794)\n",
      "\n",
      "✅ [217/41370] TEST_0217.jpg\n",
      "   Predicted: 'GRGNIER' (Confidence: 0.9726)\n",
      "\n",
      "✅ [218/41370] TEST_0218.jpg\n",
      "   Predicted: 'RANDY' (Confidence: 0.8934)\n",
      "\n",
      "✅ [219/41370] TEST_0219.jpg\n",
      "   Predicted: 'REMI' (Confidence: 0.9852)\n",
      "\n",
      "✅ [220/41370] TEST_0220.jpg\n",
      "   Predicted: 'VANMALHE' (Confidence: 0.7998)\n",
      "\n",
      "✅ [221/41370] TEST_0221.jpg\n",
      "   Predicted: 'CELINA' (Confidence: 0.9954)\n",
      "\n",
      "✅ [222/41370] TEST_0222.jpg\n",
      "   Predicted: 'BORREAUX' (Confidence: 0.9260)\n",
      "\n",
      "✅ [223/41370] TEST_0223.jpg\n",
      "   Predicted: 'SLIHADOIN' (Confidence: 0.7647)\n",
      "\n",
      "✅ [224/41370] TEST_0224.jpg\n",
      "   Predicted: 'GONES' (Confidence: 0.9884)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 318\u001b[0m\n\u001b[0;32m    315\u001b[0m img_dir \u001b[38;5;241m=\u001b[39m Config\u001b[38;5;241m.\u001b[39mTEST_IMG_DIR\n\u001b[0;32m    316\u001b[0m output_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 318\u001b[0m predict_batch(\n\u001b[0;32m    319\u001b[0m     model_path\u001b[38;5;241m=\u001b[39mConfig\u001b[38;5;241m.\u001b[39mMINI_MODEL_PATH,\n\u001b[0;32m    320\u001b[0m     image_dir\u001b[38;5;241m=\u001b[39mimg_dir,\n\u001b[0;32m    321\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39moutput_folder,\n\u001b[0;32m    322\u001b[0m     save_csv\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    323\u001b[0m )\n\u001b[0;32m    324\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;124;03m# Option 2: Uncomment to predict on a single image\u001b[39;00m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;124;03mpredict_single_image(\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;124;03m)\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 222\u001b[0m, in \u001b[0;36mpredict_batch\u001b[1;34m(model_path, image_dir, output_dir, save_csv)\u001b[0m\n\u001b[0;32m    219\u001b[0m image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_dir, filename)\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 222\u001b[0m     predicted_text, confidence \u001b[38;5;241m=\u001b[39m predict_handwritten_text(image_path, model, device)\n\u001b[0;32m    223\u001b[0m     status \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatus\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(image_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 165\u001b[0m, in \u001b[0;36mpredict_handwritten_text\u001b[1;34m(image_path, model, device)\u001b[0m\n\u001b[0;32m    162\u001b[0m image_tensor \u001b[38;5;241m=\u001b[39m transform(image)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(image_tensor)\n\u001b[0;32m    166\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    167\u001b[0m     predicted_text, confidence \u001b[38;5;241m=\u001b[39m decode_ctc(output, INT_TO_CHAR)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[1], line 95\u001b[0m, in \u001b[0;36mCRNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 95\u001b[0m     cnn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnn(x)\n\u001b[0;32m     96\u001b[0m     cnn_out \u001b[38;5;241m=\u001b[39m cnn_out\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     97\u001b[0m     cnn_out \u001b[38;5;241m=\u001b[39m cnn_out\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[0;32m    551\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "class Config:\n",
    "    CHARS = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 .,-?!&\"\n",
    "    VOCAB_SIZE = len(CHARS) + 1\n",
    "    \n",
    "    # Image Size\n",
    "    IMG_HEIGHT = 64\n",
    "    IMG_WIDTH = 256\n",
    "    HIDDEN_SIZE = 256\n",
    "    \n",
    "    # Directory Structure\n",
    "    BASE_DIR = os.getcwd()\n",
    "    CSV_DIR = os.path.join(BASE_DIR, \"CSV\")\n",
    "    TEST_IMG_DIR = os.path.join(BASE_DIR, \"test_v2/test\")\n",
    "    \n",
    "    # Annotation Files\n",
    "    TEST_CSV = os.path.join(CSV_DIR, \"written_name_test.csv\")\n",
    "    \n",
    "    # Model Paths\n",
    "    MINI_MODEL_PATH = os.path.join(BASE_DIR, \"htr_crnn_mini.pth\")\n",
    "\n",
    "\n",
    "def create_char_to_int_mapping(chars: str) -> Tuple[Dict[str, int], Dict[int, str]]:\n",
    "    char_to_int = {char: i + 1 for i, char in enumerate(chars)}\n",
    "    int_to_char = {i + 1: char for i, char in enumerate(chars)}\n",
    "    char_to_int['CTC_BLANK'] = 0\n",
    "    int_to_char[0] = ''\n",
    "    return char_to_int, int_to_char\n",
    "\n",
    "CHAR_TO_INT, INT_TO_CHAR = create_char_to_int_mapping(Config.CHARS)\n",
    "\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, img_height, vocab_size, hidden_size):\n",
    "        super(CRNN, self).__init__()\n",
    "        \n",
    "        # Improved CNN with deeper architecture and BatchNorm\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 1), (2, 1)),\n",
    "            nn.Conv2d(256, 512, 3, padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, None))\n",
    "        )\n",
    "        \n",
    "        # Map from CNN output (512 channels) to RNN input\n",
    "        self.map_to_rnn = nn.Linear(512, hidden_size)\n",
    "        \n",
    "        # Improved RNN with dropout and extra layer\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=3,\n",
    "            bidirectional=True,\n",
    "            dropout=0.3,\n",
    "            batch_first=False\n",
    "        )\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_size * 2, vocab_size)\n",
    "        \n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"Proper weight initialization\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        cnn_out = self.cnn(x)\n",
    "        cnn_out = cnn_out.squeeze(2)\n",
    "        cnn_out = cnn_out.permute(0, 2, 1)\n",
    "        \n",
    "        rnn_input = self.map_to_rnn(cnn_out)\n",
    "        rnn_input = rnn_input.permute(1, 0, 2)\n",
    "        \n",
    "        rnn_out, _ = self.rnn(rnn_input)\n",
    "        output = self.linear(rnn_out)\n",
    "        output = nn.functional.log_softmax(output, dim=2)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "def decode_ctc(output: torch.Tensor, int_to_char: Dict[int, str]) -> tuple:\n",
    "    \"\"\"\n",
    "    Decodes CTC output using greedy decoding.\n",
    "    \n",
    "    Args:\n",
    "        output: Tensor of shape [SequenceLength, VocabSize] with log probabilities\n",
    "        int_to_char: Mapping from integer indices to characters\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (decoded_text, confidence_score)\n",
    "    \"\"\"\n",
    "    probs = output.exp()\n",
    "    preds = output.argmax(dim=1)\n",
    "    max_probs = probs.max(dim=1)[0]\n",
    "    \n",
    "    decoded_text = []\n",
    "    confidence_scores = []\n",
    "    prev_idx = -1\n",
    "    \n",
    "    for i, char_idx in enumerate(preds.cpu().numpy()):\n",
    "        if char_idx != 0 and char_idx != prev_idx:\n",
    "            decoded_text.append(int_to_char.get(int(char_idx), '?'))\n",
    "            confidence_scores.append(max_probs[i].item())\n",
    "        prev_idx = char_idx\n",
    "    \n",
    "    avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0.0\n",
    "    \n",
    "    return \"\".join(decoded_text), avg_confidence\n",
    "\n",
    "\n",
    "def predict_handwritten_text(image_path: str, model: nn.Module, device) -> tuple:\n",
    "    \"\"\"\n",
    "    Predicts text from a handwritten image using a loaded model.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the image file to predict on\n",
    "        model: Already loaded CRNN model\n",
    "        device: Device to run inference on\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (predicted_text, confidence_score)\n",
    "    \"\"\"\n",
    "    if not os.path.exists(image_path):\n",
    "        raise FileNotFoundError(f\"Image file not found at: {image_path}\")\n",
    "    \n",
    "    try:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((Config.IMG_HEIGHT, Config.IMG_WIDTH)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "        \n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(image_tensor)\n",
    "            output = output.squeeze(1)\n",
    "            predicted_text, confidence = decode_ctc(output, INT_TO_CHAR)\n",
    "        \n",
    "        return predicted_text, confidence\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Prediction failed: {str(e)}\")\n",
    "\n",
    "\n",
    "def predict_batch(model_path: str, image_dir: str, output_dir: str = \"output\", save_csv: bool = True):\n",
    "    \"\"\"\n",
    "    Predicts text from all images in a directory using the mini model.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the saved model weights\n",
    "        image_dir: Directory containing images to predict on\n",
    "        output_dir: Directory to save output results\n",
    "        save_csv: Whether to save results as CSV file\n",
    "    \"\"\"\n",
    "    if not os.path.exists(image_dir):\n",
    "        print(f\"❌ Image directory not found: {image_dir}\")\n",
    "        return\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"❌ Model file not found: {model_path}\")\n",
    "        return\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\\n\")\n",
    "    \n",
    "    # Load model\n",
    "    print(\"Loading CRNN model...\")\n",
    "    model = CRNN(Config.IMG_HEIGHT, Config.VOCAB_SIZE, Config.HIDDEN_SIZE).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    print(\"✅ Model loaded successfully\\n\")\n",
    "    \n",
    "    # Get all image files\n",
    "    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff'}\n",
    "    image_files = sorted([f for f in os.listdir(image_dir) \n",
    "                          if os.path.splitext(f)[1].lower() in image_extensions])\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"❌ No image files found in {image_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(image_files)} images. Starting predictions...\\n\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for idx, filename in enumerate(image_files, 1):\n",
    "        image_path = os.path.join(image_dir, filename)\n",
    "        \n",
    "        try:\n",
    "            predicted_text, confidence = predict_handwritten_text(image_path, model, device)\n",
    "            status = \"✅\"\n",
    "            print(f\"{status} [{idx}/{len(image_files)}] {filename}\")\n",
    "            print(f\"   Predicted: '{predicted_text}' (Confidence: {confidence:.4f})\\n\")\n",
    "            \n",
    "            results.append({\n",
    "                'filename': filename,\n",
    "                'predicted_text': predicted_text,\n",
    "                'confidence': f\"{confidence:.4f}\"\n",
    "            })\n",
    "            \n",
    "        except (FileNotFoundError, RuntimeError) as e:\n",
    "            print(f\"❌ [{idx}/{len(image_files)}] {filename}\")\n",
    "            print(f\"   Error: {e}\\n\")\n",
    "            \n",
    "            results.append({\n",
    "                'filename': filename,\n",
    "                'predicted_text': 'ERROR',\n",
    "                'confidence': 'N/A'\n",
    "            })\n",
    "    \n",
    "    # Save results to CSV\n",
    "    if save_csv:\n",
    "        csv_path = os.path.join(output_dir, \"predictions.csv\")\n",
    "        try:\n",
    "            with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                fieldnames = ['filename', 'predicted_text', 'confidence']\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                writer.writeheader()\n",
    "                writer.writerows(results)\n",
    "            print(f\"✅ Results saved to: {csv_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to save CSV: {e}\")\n",
    "    \n",
    "    # Save results to text file\n",
    "    txt_path = os.path.join(output_dir, \"predictions.txt\")\n",
    "    try:\n",
    "        with open(txt_path, 'w', encoding='utf-8') as txtfile:\n",
    "            txtfile.write(\"HTR CRNN Model - Predictions\\n\")\n",
    "            txtfile.write(\"=\" * 60 + \"\\n\\n\")\n",
    "            for result in results:\n",
    "                txtfile.write(f\"File: {result['filename']}\\n\")\n",
    "                txtfile.write(f\"Predicted Text: {result['predicted_text']}\\n\")\n",
    "                txtfile.write(f\"Confidence: {result['confidence']}\\n\")\n",
    "                txtfile.write(\"-\" * 60 + \"\\n\")\n",
    "        print(f\"✅ Results saved to: {txt_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to save TXT: {e}\")\n",
    "    \n",
    "    print(f\"\\n✅ Prediction complete! Results saved in '{output_dir}' folder\")\n",
    "\n",
    "\n",
    "def predict_single_image(model_path: str, image_path: str):\n",
    "    \"\"\"\n",
    "    Predict text from a single image.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the saved model weights\n",
    "        image_path: Path to the image file\n",
    "    \"\"\"\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"❌ Model file not found: {model_path}\")\n",
    "        return\n",
    "    \n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"❌ Image file not found: {image_path}\")\n",
    "        return\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\\n\")\n",
    "    \n",
    "    # Load model\n",
    "    print(\"Loading CRNN model...\")\n",
    "    model = CRNN(Config.IMG_HEIGHT, Config.VOCAB_SIZE, Config.HIDDEN_SIZE).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    print(\"✅ Model loaded successfully\\n\")\n",
    "    \n",
    "    try:\n",
    "        predicted_text, confidence = predict_handwritten_text(image_path, model, device)\n",
    "        print(f\"Image: {os.path.basename(image_path)}\")\n",
    "        print(f\"Predicted Text: '{predicted_text}'\")\n",
    "        print(f\"Confidence: {confidence:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Prediction failed: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 60)\n",
    "    print(\"CRNN MODEL - TESTING/INFERENCE\")\n",
    "    print(\"=\" * 60 + \"\\n\")\n",
    "    #'''\n",
    "    # Option 1: Batch prediction on test directory\n",
    "    img_dir = Config.TEST_IMG_DIR\n",
    "    output_folder = \"output\"\n",
    "    \n",
    "    predict_batch(\n",
    "        model_path=Config.MINI_MODEL_PATH,\n",
    "        image_dir=img_dir,\n",
    "        output_dir=output_folder,\n",
    "        save_csv=True\n",
    "    )\n",
    "    '''\n",
    "    # Option 2: Uncomment to predict on a single image\n",
    "    predict_single_image(\n",
    "        model_path=Config.MINI_MODEL_PATH,\n",
    "        image_path=\"C:/Users/ahmed/Dropbox/PC/Pictures/Screenshots/Screenshot 2025-07-04 141651.png\"\n",
    "    )\n",
    "    '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
